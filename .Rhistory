paired = TRUE,
alternative = "two.sided")
#perform effect size --> cohen's d
d.diff<-women60s$lifesat - women50s$lifesat
xbar.d <- mean(d.diff)
sd.d <- sd(d.diff)
N.d <- length(d.diff)
d<- (xbar.d)/sd.d
cat(c("effect size", round(d,3)))
#Checking ANOVA assumptions
##Check normality of group-level data
worryNames<-c("ABOUT THE SAME"="Worry About the Same as Others",
"MORE"= "Worry More than Others",
"LESS"="Worry Less than Others")
par(mfrow = c(1,1))
ggplot(midus, aes(sample = lifesat)) +
stat_qq() +
stat_qq_line() +
facet_grid(. ~ worry, labeller = as_labeller(worryNames))+
ggtitle("Life Satisfaction of People of Different Worry Levels") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
ylab("Sample") +
xlab("Theoretical")
##Checking for homoscedasticity (equality of variances)
###visually inspecting group variance
boxplot(lifesat ~ worry, data = midus,
main="Life Satisfaction of People of Different Worry Levels",
ylab = "Life Satisfaction",
xlab = "Worry Level")
##perform the Brown-Forsythe test since center = "median"
leveneTest(lifesat ~ worry, data = midus)
# perform one way anova
mod.aov <- aov(lifesat ~ worry, data = midus)
summary(mod.aov)
worrySame <- filter(midus,worry=="ABOUT THE SAME")
worryMore <- filter(midus,worry=="MORE")
worryLess <- filter(midus,worry=="LESS")
(meanworrySame<-mean(worrySame$lifesat))
(meanworryLess<-mean(worryLess$lifesat))
(meanworryMore<-mean(worryMore$lifesat))
(sdWorrySame<- sd(worrySame$lifesat))
(sdWorryLess<- sd(worryLess$lifesat))
(sdWorryMore<- sd(worryMore$lifesat))
#ANOVA Effect Sizes
##compute n^2
mod.anova <- anova(mod.aov)
names(mod.anova)
SS <- mod.anova$Sum
MS <- mod.anova$Mean
df <- mod.anova$Df
# compute n^2
(eta2 <- SS[1]/sum(SS))
# compute w^2
(omega2 <- (SS[1] - df[1] * MS[2]) / (sum(SS) + MS[2]))
#Tukey’s HSD post-hoc pairwise tests
(mod.tuk <- TukeyHSD(mod.aov, conf.level = .95) )
#ANOVA using the lm() function
mod.lm <- lm(lifesat ~ worry, data = midus)
summary(mod.lm)
coasters <- read.csv("coasters.csv", header = TRUE)
gp<-ggplot(data=coasters, aes(x=Speed, y=Length)) +
geom_point(shape=18, color="darkblue") +
geom_smooth(method=lm, se=FALSE,color="darkred")+
ggtitle("Length vs. Speed") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
gp
#perform pearson correlation
(cor.test(x = coasters$Length, y = coasters$Speed, method = "pearson"))
#perform kendall's t
(cor.test(x = coasters$Length, y = coasters$Speed, method = "kendall"))
steel<-filter(coasters, Track=="Steel")
wood<-filter(coasters,Track=="Wood")
cat(c("n for steel:",N1<- nrow(steel)))
cat(c("n for wood:",N2<-nrow(wood)))
#correlation of wooden tracks
(cor.test(x = wood$Length, y = wood$Speed, method = "pearson", use = "complete.obs"))
#correlation of steel tracks
(cor.test(x = steel$Length, y = steel$Speed, method = "pearson", use = "complete.obs"))
#set up for r->z' transformation
r1<-cor(x = wood$Length, y = wood$Speed)
r2<-cor(x = steel$Length, y = steel$Speed)
#Convert r to z for both correlations
z1 <- fisherz(r1)
z2 <- fisherz(r2)
#Calculate z-value
z_obs <- (z1 - z2) / (sqrt(1/(N1-3)+1/(N2-3)))
cat(c("p-value = ",2 * pnorm(abs(z_obs), lower.tail = FALSE)))
# Perform a 95% CI for rho1 - rho2
CI.z <- (z1 - z2) + c(-1, 1) * qnorm(0.975) * (sqrt(1/(N1-3)+1/(N2-3)))
##transform back to r
cat(c("r confidence interval:",CI.r <- fisherz2r(CI.z)))
knitr::opts_chunk$set(echo = TRUE)
library(knitr);
library(dplyr);
library(ggplot2);
library(car);
library(scales);
library(devtools);
library(tidyverse);
library(psych);
library(binom);
library(caret);
midus <- read.csv("midus.csv", header = TRUE)
#set up
women50s<- filter(midus, age.decade == "50s")
women60s<- filter(midus, age.decade == "60s")
namesWomen = c("50s"="Women in Their 50s","60s"="Women in Their 60s")
#checking t-test assumptions
##QQ Plot to check for normality
par(mfrow = c(1,1))
ggplot(midus, aes(sample = lifesat)) +
stat_qq() +
stat_qq_line() +
facet_grid(. ~ age.decade, labeller = as_labeller(namesWomen))+
ggtitle("Life Satisfaction of Women in Different Age Groups") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
ylab("Sample") +
xlab("Theoretical")
##Boxplots to assess homogeneity of variance
par(mfrow = c(1,1))
boxplot(women50s$lifesat, women60s$lifesat,
main="Life Satisfaction of Women in Different Age Groups",
ylab = "Life Satisfaction",
names = c("Women in Their 50s", "Women in Their 60s"))
## Conduct a levene test to check for equal variance
leveneTest(midus$lifesat ~ midus$age.decade,
center = mean)
#t-test assuming alpha =0.05
t.test(women50s$lifesat, women60s$lifesat, var.equal = FALSE, alternative = "two.sided")
#set-up
N50s <- N60s <- nrow(midus)/2
var50s<- var(women50s$lifesat)
var60s<- var(women60s$lifesat)
sd.pool<- sqrt((var50s*(N50s-1)+var60s*(N60s-1))/(N50s+N60s-2))
xbar50s<- mean(women50s$lifesat)
xbar60s<- mean(women60s$lifesat)
#perform cohen's for effect size
d<- (xbar50s-xbar60s)/sd.pool
cat(c("effect size", round(d,3)))
#Paired t-test
t.test(x = women50s$lifesat, y = women60s$lifesat,
paired = TRUE,
alternative = "two.sided")
#perform effect size --> cohen's d
d.diff<-women60s$lifesat - women50s$lifesat
xbar.d <- mean(d.diff)
sd.d <- sd(d.diff)
N.d <- length(d.diff)
d<- (xbar.d)/sd.d
cat(c("effect size", round(d,3)))
#Checking ANOVA assumptions
##Check normality of group-level data
worryNames<-c("ABOUT THE SAME"="Worry About the Same as Others",
"MORE"= "Worry More than Others",
"LESS"="Worry Less than Others")
par(mfrow = c(1,1))
ggplot(midus, aes(sample = lifesat)) +
stat_qq() +
stat_qq_line() +
facet_grid(. ~ worry, labeller = as_labeller(worryNames))+
ggtitle("Life Satisfaction of People of Different Worry Levels") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
ylab("Sample") +
xlab("Theoretical")
##Checking for homoscedasticity (equality of variances)
###visually inspecting group variance
boxplot(lifesat ~ worry, data = midus,
main="Life Satisfaction of People of Different Worry Levels",
ylab = "Life Satisfaction",
xlab = "Worry Level")
##perform the Brown-Forsythe test since center = "median"
leveneTest(lifesat ~ worry, data = midus)
# perform one way anova
mod.aov <- aov(lifesat ~ worry, data = midus)
summary(mod.aov)
worrySame <- filter(midus,worry=="ABOUT THE SAME")
worryMore <- filter(midus,worry=="MORE")
worryLess <- filter(midus,worry=="LESS")
(meanworrySame<-mean(worrySame$lifesat))
(meanworryLess<-mean(worryLess$lifesat))
(meanworryMore<-mean(worryMore$lifesat))
(sdWorrySame<- sd(worrySame$lifesat))
(sdWorryLess<- sd(worryLess$lifesat))
(sdWorryMore<- sd(worryMore$lifesat))
#ANOVA Effect Sizes
##compute n^2
mod.anova <- anova(mod.aov)
names(mod.anova)
SS <- mod.anova$Sum
MS <- mod.anova$Mean
df <- mod.anova$Df
# compute n^2
(eta2 <- SS[1]/sum(SS))
# compute w^2
(omega2 <- (SS[1] - df[1] * MS[2]) / (sum(SS) + MS[2]))
#Tukey’s HSD post-hoc pairwise tests
(mod.tuk <- TukeyHSD(mod.aov, conf.level = .95) )
#ANOVA using the lm() function
mod.lm <- lm(lifesat ~ worry, data = midus)
summary(mod.lm)
coasters <- read.csv("coasters.csv", header = TRUE)
gp<-ggplot(data=coasters, aes(x=Speed, y=Length)) +
geom_point(shape=18, color="darkblue") +
geom_smooth(method=lm, se=FALSE,color="darkred")+
ggtitle("Length vs. Speed") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
gp
#perform pearson correlation
(cor.test(x = coasters$Length, y = coasters$Speed, method = "pearson"))
#perform kendall's t
(cor.test(x = coasters$Length, y = coasters$Speed, method = "kendall"))
steel<-filter(coasters, Track=="Steel")
wood<-filter(coasters,Track=="Wood")
cat(c("n for steel:",N1<- nrow(steel)))
cat(c("n for wood:",N2<-nrow(wood)))
#correlation of wooden tracks
(cor.test(x = wood$Length, y = wood$Speed, method = "pearson", use = "complete.obs"))
#correlation of steel tracks
(cor.test(x = steel$Length, y = steel$Speed, method = "pearson", use = "complete.obs"))
#set up for r->z' transformation
r1<-cor(x = wood$Length, y = wood$Speed)
r2<-cor(x = steel$Length, y = steel$Speed)
#Convert r to z for both correlations
z1 <- fisherz(r1)
z2 <- fisherz(r2)
#Calculate z-value
z_obs <- (z1 - z2) / (sqrt(1/(N1-3)+1/(N2-3)))
cat(c("p-value = ",2 * pnorm(abs(z_obs), lower.tail = FALSE)))
# Perform a 95% CI for rho1 - rho2
CI.z <- (z1 - z2) + c(-1, 1) * qnorm(0.975) * (sqrt(1/(N1-3)+1/(N2-3)))
##transform back to r
cat(c("r confidence interval:",CI.r <- fisherz2r(CI.z)))
#compute a binomial distribution to find probability
n <- nrow(coasters)
x <- sum(coasters$Inversions)
p<- .5
binom.test(x = x, n = n, p = p, alternative ="two.sided")
binom.confint(x = x, n = n, conf.level =.95, method="exact")
coaster_table<-table(coasters$Track, coasters$Inversions,
dnn = c("Track type", "Inversions"))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = TRUE,
rescale.p = TRUE))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = F,
rescale.p = TRUE))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = TRUE,
rescale.p = TRUE))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = F,
rescale.p = TRUE))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = TRUE,
rescale.p = TRUE))
odds_table <- coasters %>%
group_by(Track) %>%
summarise(odds = sum(Inversions == 1) / sum(Inversions == 0))
kable(odds_table, digits = 2, caption ="Odds of a Track Type Having an Inversion")
# create a column to display likelihood of players born in each month
baseball<-mutate(baseball, Likelihood = Ballplayer.Count/sum(Ballplayer.Count))
knitr::opts_chunk$set(echo = TRUE)
library(knitr);
library(dplyr);
library(ggplot2);
library(car);
library(scales);
library(devtools);
library(tidyverse);
library(psych);
library(binom);
library(caret);
midus <- read.csv("midus.csv", header = TRUE)
#set up
women50s<- filter(midus, age.decade == "50s")
women60s<- filter(midus, age.decade == "60s")
namesWomen = c("50s"="Women in Their 50s","60s"="Women in Their 60s")
#checking t-test assumptions
##QQ Plot to check for normality
par(mfrow = c(1,1))
ggplot(midus, aes(sample = lifesat)) +
stat_qq() +
stat_qq_line() +
facet_grid(. ~ age.decade, labeller = as_labeller(namesWomen))+
ggtitle("Life Satisfaction of Women in Different Age Groups") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
ylab("Sample") +
xlab("Theoretical")
##Boxplots to assess homogeneity of variance
par(mfrow = c(1,1))
boxplot(women50s$lifesat, women60s$lifesat,
main="Life Satisfaction of Women in Different Age Groups",
ylab = "Life Satisfaction",
names = c("Women in Their 50s", "Women in Their 60s"))
## Conduct a levene test to check for equal variance
leveneTest(midus$lifesat ~ midus$age.decade,
center = mean)
#t-test assuming alpha =0.05
t.test(women50s$lifesat, women60s$lifesat, var.equal = FALSE, alternative = "two.sided")
#set-up
N50s <- N60s <- nrow(midus)/2
var50s<- var(women50s$lifesat)
var60s<- var(women60s$lifesat)
sd.pool<- sqrt((var50s*(N50s-1)+var60s*(N60s-1))/(N50s+N60s-2))
xbar50s<- mean(women50s$lifesat)
xbar60s<- mean(women60s$lifesat)
#perform cohen's for effect size
d<- (xbar50s-xbar60s)/sd.pool
cat(c("effect size", round(d,3)))
#Paired t-test
t.test(x = women50s$lifesat, y = women60s$lifesat,
paired = TRUE,
alternative = "two.sided")
#perform effect size --> cohen's d
d.diff<-women60s$lifesat - women50s$lifesat
xbar.d <- mean(d.diff)
sd.d <- sd(d.diff)
N.d <- length(d.diff)
d<- (xbar.d)/sd.d
cat(c("effect size", round(d,3)))
#Checking ANOVA assumptions
##Check normality of group-level data
worryNames<-c("ABOUT THE SAME"="Worry About the Same as Others",
"MORE"= "Worry More than Others",
"LESS"="Worry Less than Others")
par(mfrow = c(1,1))
ggplot(midus, aes(sample = lifesat)) +
stat_qq() +
stat_qq_line() +
facet_grid(. ~ worry, labeller = as_labeller(worryNames))+
ggtitle("Life Satisfaction of People of Different Worry Levels") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
ylab("Sample") +
xlab("Theoretical")
##Checking for homoscedasticity (equality of variances)
###visually inspecting group variance
boxplot(lifesat ~ worry, data = midus,
main="Life Satisfaction of People of Different Worry Levels",
ylab = "Life Satisfaction",
xlab = "Worry Level")
##perform the Brown-Forsythe test since center = "median"
leveneTest(lifesat ~ worry, data = midus)
# perform one way anova
mod.aov <- aov(lifesat ~ worry, data = midus)
summary(mod.aov)
worrySame <- filter(midus,worry=="ABOUT THE SAME")
worryMore <- filter(midus,worry=="MORE")
worryLess <- filter(midus,worry=="LESS")
(meanworrySame<-mean(worrySame$lifesat))
(meanworryLess<-mean(worryLess$lifesat))
(meanworryMore<-mean(worryMore$lifesat))
(sdWorrySame<- sd(worrySame$lifesat))
(sdWorryLess<- sd(worryLess$lifesat))
(sdWorryMore<- sd(worryMore$lifesat))
#ANOVA Effect Sizes
##compute n^2
mod.anova <- anova(mod.aov)
names(mod.anova)
SS <- mod.anova$Sum
MS <- mod.anova$Mean
df <- mod.anova$Df
# compute n^2
(eta2 <- SS[1]/sum(SS))
# compute w^2
(omega2 <- (SS[1] - df[1] * MS[2]) / (sum(SS) + MS[2]))
#Tukey’s HSD post-hoc pairwise tests
(mod.tuk <- TukeyHSD(mod.aov, conf.level = .95) )
#ANOVA using the lm() function
mod.lm <- lm(lifesat ~ worry, data = midus)
summary(mod.lm)
coasters <- read.csv("coasters.csv", header = TRUE)
gp<-ggplot(data=coasters, aes(x=Speed, y=Length)) +
geom_point(shape=18, color="darkblue") +
geom_smooth(method=lm, se=FALSE,color="darkred")+
ggtitle("Length vs. Speed") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
gp
#perform pearson correlation
(cor.test(x = coasters$Length, y = coasters$Speed, method = "pearson"))
#perform kendall's t
(cor.test(x = coasters$Length, y = coasters$Speed, method = "kendall"))
steel<-filter(coasters, Track=="Steel")
wood<-filter(coasters,Track=="Wood")
cat(c("n for steel:",N1<- nrow(steel)))
cat(c("n for wood:",N2<-nrow(wood)))
#correlation of wooden tracks
(cor.test(x = wood$Length, y = wood$Speed, method = "pearson", use = "complete.obs"))
#correlation of steel tracks
(cor.test(x = steel$Length, y = steel$Speed, method = "pearson", use = "complete.obs"))
#set up for r->z' transformation
r1<-cor(x = wood$Length, y = wood$Speed)
r2<-cor(x = steel$Length, y = steel$Speed)
#Convert r to z for both correlations
z1 <- fisherz(r1)
z2 <- fisherz(r2)
#Calculate z-value
z_obs <- (z1 - z2) / (sqrt(1/(N1-3)+1/(N2-3)))
cat(c("p-value = ",2 * pnorm(abs(z_obs), lower.tail = FALSE)))
# Perform a 95% CI for rho1 - rho2
CI.z <- (z1 - z2) + c(-1, 1) * qnorm(0.975) * (sqrt(1/(N1-3)+1/(N2-3)))
##transform back to r
cat(c("r confidence interval:",CI.r <- fisherz2r(CI.z)))
#compute a binomial distribution to find probability
n <- nrow(coasters)
x <- sum(coasters$Inversions)
p<- .5
binom.test(x = x, n = n, p = p, alternative ="two.sided")
binom.confint(x = x, n = n, conf.level =.95, method="exact")
coaster_table<-table(coasters$Track, coasters$Inversions,
dnn = c("Track type", "Inversions"))
#set-up for chi squared test
coaster_table <- as.matrix(coaster_table)
#conduct chi-squared test
(ind_test <- chisq.test(x = coaster_table,
correct = TRUE,
rescale.p = TRUE))
odds_table <- coasters %>%
group_by(Track) %>%
summarise(odds = sum(Inversions == 1) / sum(Inversions == 0))
kable(odds_table, digits = 2, caption ="Odds of a Track Type Having an Inversion")
baseball <- read.csv("baseball.csv", header = TRUE)
head(baseball)
# create a column to display likelihood of players born in each month
baseball<-mutate(baseball, Likelihood = Ballplayer.Count/sum(Ballplayer.Count))
colnames(baseball) <- c("Count", "Month","Likelihood")
#conduct a chi squared
(ind_test <- chisq.test(x = baseball$Count,
correct = FALSE,
rescale.p = TRUE))
kable(baseball,caption = "number of baseball players \nborn in each month\n and their likelihood")
# create a column to display likelihood of players born in each month
baseball<-mutate(baseball, Likelihood = Ballplayer.Count/sum(Ballplayer.Count))
baseball <- read.csv("baseball.csv", header = TRUE)
head(baseball)
# create a column to display likelihood of players born in each month
baseball<-mutate(baseball, Likelihood = Ballplayer.Count/sum(Ballplayer.Count))
colnames(baseball) <- c("Count", "Month","Likelihood")
#conduct a chi squared
(ind_test2 <- chisq.test(x = baseball$Count,
correct = FALSE,
rescale.p = TRUE))
kable(baseball,caption = "number of baseball players \nborn in each month\n and their likelihood")
#calculate difference
difference<- ind_test2$observed-ind_test2$expected
new_dat<-data.frame(baseball$Month,difference)
#calculate difference
difference<- ind_test2$observed-ind_test2$expected
new_dat<-data.frame(baseball$Month,difference)
kable(new_dat)
#calculate difference
Month<-baseball$Month
difference<- ind_test2$observed-ind_test2$expected
new_dat<-data.frame(Month,difference)
kable(new_dat)
#calculate difference
Month<-baseball$Month
Difference<- ind_test2$observed-ind_test2$expected
new_dat<-data.frame(Month,Difference)
kable(new_dat,caption = "The Difference of Unstandardized between the Expected and Observed counts")
#calculate difference
Month<-baseball$Month
Difference<- ind_test2$observed-ind_test2$expected
new_dat<-data.frame(Month,Difference)
kable(new_dat,caption = "The Unstandardized Difference between the Expected and Observed counts")
> smoke
colnames(smoke) <- c("High","Low","Middle")
smoke <- as.table(smoke)
source('~/.active-rstudio-document')
getwd
getwd()
setwd("/Users/navar135/Documents/UMN_Research/Projects/Diplopia")
yLocation <- c(470,555,595,545,500,660,490,570,530,330,
430,525,575,440,810,640,700,650,650,575)
locations
#create table
locations <- matrix(c(ImageNames,xLocation,yLocation),ncol=3,byrow=TRUE)
ImageNames <- c("Truck1","Truck2","Truck3","Truck4","Truck5",
"Truck6","Truck7","Truck8","Truck9","Truck10",
"Truck11","Truck12","Truck13","Truck14","Truck15",
"Truck16","Truck17","Truck18","Truck19","Truck20")
xLocation <- c(490,420,820,440,550,560,560,450,545,575,
405,560,615,690,640,430,720,580,460,660)
yLocation <- c(470,555,595,545,500,660,490,570,530,330,
430,525,575,440,810,640,700,650,650,575)
locations <- matrix(c(ImageNames,xLocation,yLocation),ncol=3,byrow=TRUE)
colnames(locations) <- c("Image","x","y")
locations <- as.table(locations)
locations
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
install.packages("R.matlab")
library("R.matlab", lib.loc="~/Library/R/3.5/library")
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
source('~/Documents/UMN_Research/Projects/Diplopia/gabor/GaborLocations.R')
